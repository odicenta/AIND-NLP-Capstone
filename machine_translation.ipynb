{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "## Machine Translation Project\n",
    "In this notebook, sections that end with **'(IMPLEMENTATION)'** in the header indicate that the following blocks of code will require additional functionality which you must provide. Please be sure to read the instructions carefully!\n",
    "\n",
    "## Introduction\n",
    "In this notebook, you will build a deep neural network that functions as part of an end-to-end machine translation pipeline. Your completed pipeline will accept English text as input and return the French translation.\n",
    "\n",
    "- **Preprocess** - You'll convert text to sequence of integers.\n",
    "- **Models** Create models which accepts a sequence of integers as input and returns a probability distribution over possible translations. After learning about the basic types of neural networks that are often used for machine translation, you will engage in your own investigations, to design your own model!\n",
    "- **Prediction** Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU\n",
    "The following test applies only if you expect to be using a GPU, e.g., while running in a Udacity Workspace or using an AWS instance with GPU support. Run the next cell, and verify that the device_type is \"GPU\".\n",
    "- If the device is not GPU & you are running from a Udacity Workspace, then save your workspace with the icon at the top, then click \"enable\" at the bottom of the workspace.\n",
    "- If the device is not GPU & you are running from an AWS instance, then refer to the cloud computing instructions in the classroom to verify your setup steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16062957568953322278\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 357302272\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12496100525209309083\n",
      "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  However, that will take a long time to train a neural network on.  We'll be using a dataset we created for this project that contains a small vocabulary.  You'll be able to train your model in a reasonable time with this dataset.\n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at the sentences, you can see they have been preprocessed already.  The puncuations have been delimited using spaces. All the text have been converted to lowercase.  This should save you some time, but the text requires more preprocessing.\n",
    "### Vocabulary\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words.\n",
    "## Preprocess\n",
    "For this project, you won't use text data as input to your model. Instead, you'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids\n",
    "2. Add padding to make all the sequences the same length.\n",
    "\n",
    "Time to start preprocessing the data...\n",
    "### Tokenize (IMPLEMENTATION)\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings.  Since a neural network is a series of multiplication and addition operations, the input data needs to be number(s).\n",
    "\n",
    "We can turn each character into a number or each word into a number.  These are called character and word ids, respectively.  Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word.  Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "Turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function. Use this function to tokenize `english_sentences` and `french_sentences` in the cell below.\n",
    "\n",
    "Running the cell will run `tokenize` on sample data and show output for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # Create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # Assign the tokens\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding (IMPLEMENTATION)\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length.  Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length.\n",
    "\n",
    "Make sure all the English sequences have the same length and all the French sequences have the same length by adding padding to the **end** of each sequence using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    padded = pad_sequences(x, maxlen=length, padding='post')\n",
    "    return padded\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Pipeline\n",
    "Your focus for this project is to build neural network architecture, so we won't ask you to create a preprocess pipeline.  Instead, we've provided you with the implementation of the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "In this section, you will experiment with various neural network architectures.\n",
    "You will begin by training four relatively simple architectures.\n",
    "- Model 1 is a simple RNN\n",
    "- Model 2 is a RNN with Embedding\n",
    "- Model 3 is a Bidirectional RNN\n",
    "- Model 4 is an optional Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: RNN (IMPLEMENTATION)\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data.  In this model, you'll build a RNN that translates English to French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 7s 68us/step - loss: 2.1451 - acc: 0.5256 - val_loss: nan - val_acc: 0.6048\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 7s 64us/step - loss: 1.3889 - acc: 0.6242 - val_loss: nan - val_acc: 0.6414\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 7s 65us/step - loss: 1.2079 - acc: 0.6518 - val_loss: nan - val_acc: 0.6571\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 7s 66us/step - loss: 1.1193 - acc: 0.6605 - val_loss: nan - val_acc: 0.6604\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 7s 66us/step - loss: 1.0621 - acc: 0.6681 - val_loss: nan - val_acc: 0.6699\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 7s 65us/step - loss: 1.0149 - acc: 0.6761 - val_loss: nan - val_acc: 0.6811\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 7s 64us/step - loss: 0.9786 - acc: 0.6842 - val_loss: nan - val_acc: 0.6856\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 7s 64us/step - loss: 0.9494 - acc: 0.6915 - val_loss: nan - val_acc: 0.6924\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 7s 64us/step - loss: 0.9238 - acc: 0.7004 - val_loss: nan - val_acc: 0.7077\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 7s 64us/step - loss: 0.9000 - acc: 0.7100 - val_loss: nan - val_acc: 0.7169\n",
      "new jersey est parfois chaud en mois et il et il est en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Build the layers\n",
    "    inputs = Input(shape=input_shape[1:]) # Needs to return an iterable, even if dim 1\n",
    "    # We need the full output of the hidden layer to pass along\n",
    "    hidden = GRU(128, activation='tanh', return_sequences=True)(inputs)\n",
    "    outputs = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(hidden)\n",
    "    \n",
    "    # define a learning rate (hyperparameter)\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "tests.test_simple_model(simple_model)\n",
    "\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "The basic model was the same for all the different tests being done. The learning rate has been changed from 0.001 to 0.1 in a factor of 10 increments to observe the sensitivity of the model to the different values of learning rates. The findings are shown below:\n",
    "\n",
    "1. Learning rate = 0.001 - Accuracy: 48%. Translation does not make sense and repeats 'est' several times, probably as being one of the most used words\n",
    "2. Learning rate = 0.01 - Accuracy: 61%. Translation is not too bad although once starts missing the words, it kinds of starts repeating itself (\"new jersey est parfois parfois en mois et il est il en en\"). There is definetely some more variation in the word choices, but still not good.\n",
    "3. Learning rate = 0.1 - Accuracy: 61%. Translation seems about the same as the previous one, although the words choice is different in this case. Recurrent and repeated words are about the same, the most frequent (\"new jersey est parfois parfois en printemps et il il est en en\"). \n",
    "\n",
    "This last one seems to make more sense, but still, we have rerun and kept lr = 0.01 for a couple of reasons. The first one, in general terms a large learning rate is more prone to jump from one solution to another, missing local minima, and the second one is that there is no gain from making it bigger, even if the occassional translation seems better, using accuracy as the real measurement in this case.\n",
    "\n",
    "### Update after review\n",
    "Updating the parameters to the suggestion from the reviewer (lower the learning rate at the same time than incresing the GRU units from output_sequence_length to 128) improved the results to an **accuracy of 70% with a translation:** *new jersey est parfois chaud en mois et il et il est en en* that is indeed better than before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding (IMPLEMENTATION)\n",
    "![RNN](images/embedding.png)\n",
    "You've turned the words into ids, but there's a better representation of a word.  This is called word embeddings.  An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In this model, you'll create a RNN model using embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/25\n",
      "110288/110288 [==============================] - 8s 75us/step - loss: 4.3184 - acc: 0.4085 - val_loss: nan - val_acc: 0.4179\n",
      "Epoch 2/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 2.9779 - acc: 0.4269 - val_loss: nan - val_acc: 0.4605\n",
      "Epoch 3/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 2.2069 - acc: 0.5470 - val_loss: nan - val_acc: 0.6232\n",
      "Epoch 4/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 1.6555 - acc: 0.6489 - val_loss: nan - val_acc: 0.6668\n",
      "Epoch 5/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 1.3931 - acc: 0.6761 - val_loss: nan - val_acc: 0.6821\n",
      "Epoch 6/25\n",
      "110288/110288 [==============================] - 8s 68us/step - loss: 1.2370 - acc: 0.6862 - val_loss: nan - val_acc: 0.6937\n",
      "Epoch 7/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 1.1204 - acc: 0.7065 - val_loss: nan - val_acc: 0.7206\n",
      "Epoch 8/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 1.0168 - acc: 0.7358 - val_loss: nan - val_acc: 0.7505\n",
      "Epoch 9/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.9344 - acc: 0.7614 - val_loss: nan - val_acc: 0.7724\n",
      "Epoch 10/25\n",
      "110288/110288 [==============================] - 7s 67us/step - loss: 0.8730 - acc: 0.7776 - val_loss: nan - val_acc: 0.7854\n",
      "Epoch 11/25\n",
      "110288/110288 [==============================] - 7s 68us/step - loss: 0.8256 - acc: 0.7892 - val_loss: nan - val_acc: 0.7935\n",
      "Epoch 12/25\n",
      "110288/110288 [==============================] - 8s 68us/step - loss: 0.7879 - acc: 0.7972 - val_loss: nan - val_acc: 0.7998\n",
      "Epoch 13/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.7576 - acc: 0.8035 - val_loss: nan - val_acc: 0.8068\n",
      "Epoch 14/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.7318 - acc: 0.8090 - val_loss: nan - val_acc: 0.8121\n",
      "Epoch 15/25\n",
      "110288/110288 [==============================] - 8s 71us/step - loss: 0.7097 - acc: 0.8133 - val_loss: nan - val_acc: 0.8137\n",
      "Epoch 16/25\n",
      "110288/110288 [==============================] - 8s 71us/step - loss: 0.6903 - acc: 0.8163 - val_loss: nan - val_acc: 0.8195\n",
      "Epoch 17/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.6725 - acc: 0.8199 - val_loss: nan - val_acc: 0.8211\n",
      "Epoch 18/25\n",
      "110288/110288 [==============================] - 8s 70us/step - loss: 0.6567 - acc: 0.8233 - val_loss: nan - val_acc: 0.8231\n",
      "Epoch 19/25\n",
      "110288/110288 [==============================] - 8s 70us/step - loss: 0.6428 - acc: 0.8265 - val_loss: nan - val_acc: 0.8286\n",
      "Epoch 20/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.6290 - acc: 0.8301 - val_loss: nan - val_acc: 0.8314\n",
      "Epoch 21/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.6172 - acc: 0.8329 - val_loss: nan - val_acc: 0.8341\n",
      "Epoch 22/25\n",
      "110288/110288 [==============================] - 8s 70us/step - loss: 0.6056 - acc: 0.8352 - val_loss: nan - val_acc: 0.8365\n",
      "Epoch 23/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.5945 - acc: 0.8374 - val_loss: nan - val_acc: 0.8374\n",
      "Epoch 24/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.5844 - acc: 0.8392 - val_loss: nan - val_acc: 0.8402\n",
      "Epoch 25/25\n",
      "110288/110288 [==============================] - 8s 69us/step - loss: 0.5751 - acc: 0.8411 - val_loss: nan - val_acc: 0.8423\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 21, 256)           50944     \n",
      "_________________________________________________________________\n",
      "gru_12 (GRU)                 (None, 21, 21)            17514     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 21, 344)           7568      \n",
      "=================================================================\n",
      "Total params: 76,026\n",
      "Trainable params: 76,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Translate 'new jersey is sometimes quiet during autumn , and it is snowy in april .': \n",
      "\n",
      "new jersey est parfois calme en l' et il il la la en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Implement, by following a similar process as the previous one\n",
    "    # Build the layers\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    # The total size of the embeddings space is that of the vocab size, the output is equal\n",
    "    # to the sentence we are processing and same dim as the next layer\n",
    "    embeds = Embedding(input_dim=english_vocab_size,\n",
    "                       output_dim=256)(inputs)\n",
    "    # The rest of the model is similar, just we operate with the embeddings now\n",
    "    hidden = GRU(output_sequence_length, activation='tanh', return_sequences=True)(embeds)\n",
    "    outputs = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(hidden)\n",
    "    \n",
    "    # define a learning rate (hyperparameter)\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # Build the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tests.test_embed_model(embed_model)\n",
    "\n",
    "\n",
    "# Reshape the input, now the embeds have a different dimension as we can feed the embeddings and\n",
    "# not word by word\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2]))\n",
    "\n",
    "# Train the neural network\n",
    "simple_embed_model = embed_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_embed_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=25, validation_split=0.2)\n",
    "\n",
    "# Show a summary before testing prediction\n",
    "simple_embed_model.summary()\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Translate '{}': \\n\".format(english_sentences[0]))\n",
    "print(logits_to_text(simple_embed_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "A similar analysis to the previous one is performed again with this model:\n",
    "\n",
    "1. Learning rate = 0.001 - Accuracy: 82% after 30 epochs, as with only 10, the network had low accuracy but it was still steadly learning, so it was extended. Translation after that does not seem as good as the others \"new jersey est parfois calme en l' et il est est en en\" falling again into the repetition of the most frequent words, probably just an artifact of this example, given the accuracy, or maybe due to not properly separating testing from validation. \n",
    "2. Learning rate = 0.01 - Accuracy: 84%. Translation is pretty good although with some 'hesitation' on part of the model, repeating some common words. \"new jersey est parfois calme en cours **et et il la en en** avril\". Accuracy seems to have stabilized after 10 epochs\n",
    "3. Learning rate = 0.1 - Accuracy: 83%. Translation seems a little bit worse than the previous one, although the accuracy is similar, it may be an isolated case, or an overfitting of problems to find the local minima,  (\"new jersey est parfois calme en l' et il est la neigeux en\"). Accuracy seems to stabilize after half the 10 epochs, not learning much afterwards\n",
    "\n",
    "In general terms, this is a much better model, where the embedding helps the model to find relationships much faster than the previous one. Again, the version of 0.01 learning rate has been selected for best results and example of translation.\n",
    "\n",
    "### Update after review\n",
    "Updating the parameters to the suggestion from the reviewer (lower the learning rate at the same time than incresing the GRU units from output_sequence_length to 256 in this case, as it seems that 128 makes the model to fall into an accuracy trap where loss is no longer diminished after a certain point) improved the results to an **accuracy of 84% (barely an improvement in this case, as the embedding seems to do a really good job with very few units) with a translation:** *new jersey est parfois calme en l' automne et il est neigeux en avril* that is a really good one.\n",
    "\n",
    "In this case, a learning rate of 0.005 reached the 84% in the suggested 10 epochs, but in order to reach that value with a slower learning rate, 25 epochs were needed to get basically the same result and a worse example translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs (IMPLEMENTATION)\n",
    "![RNN](images/bidirectional.png)\n",
    "One restriction of a RNN is that it can't see the future input, only the past.  This is where bidirectional recurrent neural networks come in.  They are able to see the future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 13s 115us/step - loss: 1.7740 - acc: 0.5881 - val_loss: nan - val_acc: 0.6473\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 12s 104us/step - loss: 1.1460 - acc: 0.6667 - val_loss: nan - val_acc: 0.6818\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 12s 104us/step - loss: 1.0042 - acc: 0.6885 - val_loss: nan - val_acc: 0.6960\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 12s 104us/step - loss: 0.9206 - acc: 0.7016 - val_loss: nan - val_acc: 0.7066\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 11s 103us/step - loss: 0.8699 - acc: 0.7104 - val_loss: nan - val_acc: 0.7148\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 11s 103us/step - loss: 0.8369 - acc: 0.7165 - val_loss: nan - val_acc: 0.7053\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 12s 105us/step - loss: 0.8055 - acc: 0.7241 - val_loss: nan - val_acc: 0.7326\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 12s 105us/step - loss: 0.7690 - acc: 0.7333 - val_loss: nan - val_acc: 0.7309\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 12s 105us/step - loss: 0.7495 - acc: 0.7391 - val_loss: nan - val_acc: 0.7434\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 12s 105us/step - loss: 0.7208 - acc: 0.7495 - val_loss: nan - val_acc: 0.7545\n",
      "Translate 'new jersey is sometimes quiet during autumn , and it is snowy in april .': \n",
      "\n",
      "new jersey est parfois calme au mois de il il il chaud en en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Implementation similar to the original model, just using the bidirectional layer\n",
    "    \n",
    "    # Build the layers\n",
    "    inputs = Input(shape=input_shape[1:]) # Needs to return an iterable, even if dim 1\n",
    "    # We need the full output of the hidden layer to pass along\n",
    "    bd = Bidirectional(GRU(128, activation='tanh', return_sequences=True), merge_mode='concat')(inputs)\n",
    "    outputs = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(bd)\n",
    "    \n",
    "    # define a learning rate (hyperparameter)\n",
    "    learning_rate = 0.005\n",
    "\n",
    "    # Build the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "tests.test_bd_model(bd_model)\n",
    "\n",
    "# Train and Print prediction(s)\n",
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = pad(preproc_english_sentences, max_french_sequence_length)\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_bd_model = bd_model(\n",
    "    tmp_x.shape,\n",
    "    max_french_sequence_length,\n",
    "    english_vocab_size,\n",
    "    french_vocab_size)\n",
    "simple_bd_model.fit(tmp_x, preproc_french_sentences, batch_size=1024, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print(\"Translate '{}': \\n\".format(english_sentences[0]))\n",
    "print(logits_to_text(simple_bd_model.predict(tmp_x[:1])[0], french_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "A similar analysis to the previous ones is performed again with the bidirectional model:\n",
    "\n",
    "1. Learning rate = 0.001 - Accuracy: 67% after 50 epochs, as with only 10, steady growth till epoch 10, with 60% accuracy, but after that it flattens around 65 in about 5 more epochs, very little progress from there. The translation is not good *\"new jersey est parfois calme en l' et il est est en en\"* again, falling into the repetition trap.\n",
    "2. Learning rate = 0.01 - Accuracy: 67%. with 65% acc reached by epoch 5 and flattening from then on. Translation is about the same quality as above, with same caveats. *\"new jersey est parfois calme en mois et il est est en en\"*. \n",
    "3. Learning rate = 0.1 - Accuracy: 64%. Translation seems a little bit worse than the previous one, with slightly worse accuracy too,  *\"new jersey est parfois calme en en et il est est en en \"*. Accuracy seems to stabilize after 3-4 epochs, but lack precision, not learning much afterwards\n",
    "\n",
    "In general terms, this is a much better model than the first one, but not as good as the second. Still, it shows there are other techniques that we could apply to the basic RNN that improves its capabilities. Only thing left would be check if by combining both techniques, we can take advantage of both, seeing the past and the future, and the abstraction of embedding to facilitate mathematical operations.\n",
    "\n",
    "### Update after review\n",
    "Updating the parameters to the suggestion from the reviewer (lower the learning rate at the same time than incresing the GRU units from output_sequence_length to 128) improved the results to an **accuracy of 75% with a translation:** *new jersey est parfois calme au mois de il il il chaud en en*. Quite an improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder (OPTIONAL)\n",
    "Time to look at encoder-decoder models.  This model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence.  The decoder takes this matrix as input and predicts the translation as output.\n",
    "\n",
    "Create an encoder-decoder model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # OPTIONAL: Implement\n",
    "    return None\n",
    "tests.test_encdec_model(encdec_model)\n",
    "\n",
    "\n",
    "# OPTIONAL: Train and Print prediction(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Custom (IMPLEMENTATION)\n",
    "Use everything you learned from the previous models to create a model that incorporates embedding and a bidirectional rnn into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 128)           25472     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               197376    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 21, 512)           787968    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 21, 688)           352944    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 21, 344)           237016    \n",
      "=================================================================\n",
      "Total params: 1,600,776\n",
      "Trainable params: 1,600,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Final Model Loaded\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Implement, by following a similar process as before\n",
    "    # Build the layers\n",
    "    inputs = Input(shape=input_shape[1:])\n",
    "    embeds = Embedding(input_dim=english_vocab_size,\n",
    "                       output_dim=128)(inputs)\n",
    "    # The rest of the model is similar, just we operate with the embeddings now\n",
    "    hidden_bd = Bidirectional(GRU(128, activation='tanh',\n",
    "                                  return_sequences=False,\n",
    "                                  dropout = 0.3, recurrent_dropout=0.3))(embeds)\n",
    "    hidden_bd = RepeatVector(output_sequence_length)(hidden_bd)\n",
    "    hidden_bd = Bidirectional(GRU(256,\n",
    "                                  activation='tanh',\n",
    "                                  return_sequences=True,\n",
    "                                  dropout = 0.3, recurrent_dropout=0.3))(hidden_bd)\n",
    "    hidden_bd = TimeDistributed(Dense(2 * french_vocab_size, activation='relu'))(hidden_bd)\n",
    "    outputs = TimeDistributed(Dense(french_vocab_size, activation='softmax'))(hidden_bd)\n",
    "    \n",
    "    # define a learning rate (hyperparameter)\n",
    "    learning_rate = 0.005\n",
    "    \n",
    "    # Build the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "tests.test_model_final(model_final)\n",
    "    \n",
    "print('Final Model Loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction (IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 21, 128)           25600     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 256)               197376    \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 21, 512)           787968    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 21, 690)           353970    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 345)           238395    \n",
      "=================================================================\n",
      "Total params: 1,603,309\n",
      "Trainable params: 1,603,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/35\n",
      "110288/110288 [==============================] - 42s 381us/step - loss: 2.0786 - acc: 0.5283 - val_loss: 1.2492 - val_acc: 0.6660\n",
      "Epoch 2/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 1.2062 - acc: 0.6673 - val_loss: 1.1517 - val_acc: 0.6797\n",
      "Epoch 3/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.9822 - acc: 0.7217 - val_loss: 0.8287 - val_acc: 0.7602\n",
      "Epoch 4/35\n",
      "110288/110288 [==============================] - 39s 353us/step - loss: 0.8105 - acc: 0.7583 - val_loss: 0.6370 - val_acc: 0.8065\n",
      "Epoch 5/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.6438 - acc: 0.8011 - val_loss: 0.4891 - val_acc: 0.8499\n",
      "Epoch 6/35\n",
      "110288/110288 [==============================] - 39s 351us/step - loss: 0.5298 - acc: 0.8356 - val_loss: 0.3938 - val_acc: 0.8832\n",
      "Epoch 7/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.4266 - acc: 0.8659 - val_loss: 0.2743 - val_acc: 0.9189\n",
      "Epoch 8/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.3344 - acc: 0.8945 - val_loss: 0.2210 - val_acc: 0.9354\n",
      "Epoch 9/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.2773 - acc: 0.9129 - val_loss: 0.1795 - val_acc: 0.9466\n",
      "Epoch 10/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.2470 - acc: 0.9220 - val_loss: 0.1583 - val_acc: 0.9529\n",
      "Epoch 11/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.2165 - acc: 0.9316 - val_loss: 0.1507 - val_acc: 0.9545\n",
      "Epoch 12/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1982 - acc: 0.9373 - val_loss: 0.1316 - val_acc: 0.9604\n",
      "Epoch 13/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1875 - acc: 0.9407 - val_loss: 0.1231 - val_acc: 0.9630\n",
      "Epoch 14/35\n",
      "110288/110288 [==============================] - 39s 351us/step - loss: 0.1733 - acc: 0.9453 - val_loss: 0.1184 - val_acc: 0.9651\n",
      "Epoch 15/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1647 - acc: 0.9481 - val_loss: 0.1160 - val_acc: 0.9656\n",
      "Epoch 16/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1599 - acc: 0.9497 - val_loss: 0.1084 - val_acc: 0.9680\n",
      "Epoch 17/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1539 - acc: 0.9517 - val_loss: 0.1075 - val_acc: 0.9677\n",
      "Epoch 18/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1478 - acc: 0.9535 - val_loss: 0.1032 - val_acc: 0.9694\n",
      "Epoch 19/35\n",
      "110288/110288 [==============================] - 39s 353us/step - loss: 0.1502 - acc: 0.9531 - val_loss: 0.1367 - val_acc: 0.9590\n",
      "Epoch 20/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1888 - acc: 0.9400 - val_loss: 0.1067 - val_acc: 0.9681\n",
      "Epoch 21/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1510 - acc: 0.9523 - val_loss: 0.0989 - val_acc: 0.9708\n",
      "Epoch 22/35\n",
      "110288/110288 [==============================] - 39s 351us/step - loss: 0.1404 - acc: 0.9555 - val_loss: 0.0944 - val_acc: 0.9720\n",
      "Epoch 23/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1374 - acc: 0.9567 - val_loss: 0.0968 - val_acc: 0.9714\n",
      "Epoch 24/35\n",
      "110288/110288 [==============================] - 39s 351us/step - loss: 0.1354 - acc: 0.9574 - val_loss: 0.0940 - val_acc: 0.9725\n",
      "Epoch 25/35\n",
      "110288/110288 [==============================] - 39s 351us/step - loss: 0.1296 - acc: 0.9592 - val_loss: 0.0922 - val_acc: 0.9733\n",
      "Epoch 26/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1268 - acc: 0.9601 - val_loss: 0.0906 - val_acc: 0.9732\n",
      "Epoch 27/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1232 - acc: 0.9609 - val_loss: 0.0869 - val_acc: 0.9745\n",
      "Epoch 28/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1237 - acc: 0.9611 - val_loss: 0.0875 - val_acc: 0.9744\n",
      "Epoch 29/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1220 - acc: 0.9617 - val_loss: 0.0893 - val_acc: 0.9737\n",
      "Epoch 30/35\n",
      "110288/110288 [==============================] - 39s 351us/step - loss: 0.1209 - acc: 0.9620 - val_loss: 0.0835 - val_acc: 0.9758\n",
      "Epoch 31/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1197 - acc: 0.9623 - val_loss: 0.0859 - val_acc: 0.9746\n",
      "Epoch 32/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1181 - acc: 0.9629 - val_loss: 0.0841 - val_acc: 0.9753\n",
      "Epoch 33/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1167 - acc: 0.9632 - val_loss: 0.0848 - val_acc: 0.9754\n",
      "Epoch 34/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1190 - acc: 0.9627 - val_loss: 0.0823 - val_acc: 0.9758\n",
      "Epoch 35/35\n",
      "110288/110288 [==============================] - 39s 352us/step - loss: 0.1132 - acc: 0.9643 - val_loss: 0.0813 - val_acc: 0.9769\n",
      "Sample 1:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Il a vu un vieux camion jaune\n",
      "Sample 2:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def final_predictions(x, y, x_tk, y_tk):\n",
    "    \"\"\"\n",
    "    Gets predictions using the final model\n",
    "    :param x: Preprocessed English data\n",
    "    :param y: Preprocessed French data\n",
    "    :param x_tk: English tokenizer\n",
    "    :param y_tk: French tokenizer\n",
    "    \"\"\"\n",
    "    # Train neural network using model_final\n",
    "    \n",
    "    # Pad the input \n",
    "    x = pad(x, y.shape[1])\n",
    "    \n",
    "    # Build the model\n",
    "    model = model_final(x.shape, y.shape[1], len(x_tk.word_counts)+1, len(y_tk.word_counts)+1)\n",
    "    \n",
    "    # Fit the model with the data\n",
    "    model.fit(x, y, batch_size=1024, epochs=35, validation_split=0.2)\n",
    "    \n",
    "    ## DON'T EDIT ANYTHING BELOW THIS LINE\n",
    "    y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "    y_id_to_word[0] = '<PAD>'\n",
    "\n",
    "    sentence = 'he saw a old yellow truck'\n",
    "    sentence = [x_tk.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad_sequences([sentence], maxlen=x.shape[-1], padding='post')\n",
    "    sentences = np.array([sentence[0], x[0]])\n",
    "    predictions = model.predict(sentences, len(sentences))\n",
    "\n",
    "    print('Sample 1:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[0]]))\n",
    "    print('Il a vu un vieux camion jaune')\n",
    "    print('Sample 2:')\n",
    "    print(' '.join([y_id_to_word[np.argmax(x)] for x in predictions[1]]))\n",
    "    print(' '.join([y_id_to_word[np.max(x)] for x in y[0]]))\n",
    "\n",
    "\n",
    "final_predictions(preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Results\n",
    "\n",
    "In this case, the approach to the network architecture is slightly different. \n",
    "\n",
    "In a first shot, a try was done incorporating both the Bidirectional and the embeddings, with basically the same parameters as the previous networks. This was done because until this point, the intent behind using the same basic architecture was to be able to compare results using the same ideas.\n",
    "\n",
    "The resultant networks allowed to assess both the impact of the basic idea in a simple way, but this was it, it was a simple network as reflected by the summary of the model (about 15 to 30 thousand different parameters). So when combining all the different ideas, the number of parameters that the network was able to incorporate was not able to capture the complexity of the problem. This resulted in several effects:\n",
    "\n",
    "- Slow training: Suddenly, the 10 epochs even with the standard learning rate of 0.1 provided a steady but slower learning than in previous architectures.\n",
    "- Worse results than the simpler solutions: Even after adding epochs (as much as 100) the network topped at 83% validation accuracy. It was still growing when cut off, but it came to a crawl and would have needed at least 500 in order to improve 3-4% and be closer to 90% desired. This result is worse than the embeddings, with a much lower requirement in computational power.\n",
    "\n",
    "So there was the need to modify the model in order to accomodate the added complexity provided by both techniques. \n",
    "\n",
    "A first attemp previous to introducing additional complexity was using dropout. By normalizing the network and allowing some \"forgetfulness\", it was expected than the last epochs, where the variation of loss and validation was very small, improved, and it was indeed, but the gains very very modest (from 82% to 84% approx.). Even more so, this was not consistent and there seemed to be a sweet spot where the dropout value gave the biggest improvements (dropout = 0.2) but larger or smaller values resulted in no improvement, to slower learning -as it was in part expected- that resulted in same or worse results after 100 epochs.\n",
    "\n",
    "As this was not enough and there were still some issues with the translations *\"new jersey est parfois chaud pendant l' automne et il est neigeux en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\"* (missing the last word) a new dense layer was introduced before the last one that allowed the model to abstract a larger number of concepts, changing the number of parameters from 30 thousand to almost half a million.\n",
    "    \n",
    "But training it was a different issue, as the network learned really fast, enough to go above 90% validity accuracy after only 30 epochs, but with some glitches:\n",
    "\n",
    "- Validity accuracy (and loss) was always around 4-5% better (higher for accuracy, lower for loss) than training, consistently, over all the epochs. This was unexpected and normally the problem is the inverse where there is overtraining.\n",
    "- After certain point, the system became \"amnesiac\", for a lack of a better word to describe it, as it showed an increased loss with every step at certain point that only stopped once a loss of about 3 or 4 was established. The best guess is that the gradient changed signs making the descent become \"ascent\" until it corrected itself. After that point, recovery was not reached, or not at the same pace. The solution has been to cut the fitting before this state was reached, but letting it go further, it consistently reached the amnesiac state with the network shown, after about 40-50 epochs.\n",
    "\n",
    "As a summary, including new ideas has consistently improved the model and the results, although it required the addition of new layers or techniques and more care in the training before reaching satisfactory results, and encorauges to learn more about the issues encountered during the exercise.\n",
    "\n",
    "### Update after review\n",
    "By adopting the suggestions from the reviewer, the accuracy reached the desired levels and the issue present related to the sudden loss of accuracy didn't present again, obtaining **accuracy of 97.7% with an accurate translation for both sentences**. The greater gains are as usual located in the first epochs (7-8) and after than 95% is obtained after 10 epochs. Another 25 barely added a couple of percentage points, but these are the ones that manage to get accurate translations almost all the time compared to having always one or two words off.\n",
    "\n",
    "It is interesting to notice that the loss and accuracy for both training and validation sets are more in accordance now, compared to the previous scenario causing the issue described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "When you're ready to submit, complete the following steps:\n",
    "1. Review the [rubric](https://review.udacity.com/#!/rubrics/1004/view) to ensure your submission meets all requirements to pass\n",
    "2. Generate an HTML version of this notebook\n",
    "\n",
    "  - Run the next cell to attempt automatic generation (this is the recommended method in Workspaces)\n",
    "  - Navigate to **FILE -> Download as -> HTML (.html)**\n",
    "  - Manually generate a copy using `nbconvert` from your shell terminal\n",
    "```\n",
    "$ pip install nbconvert\n",
    "$ python -m nbconvert machine_translation.ipynb\n",
    "```\n",
    "  \n",
    "3. Submit the project\n",
    "\n",
    "  - If you are in a Workspace, simply click the \"Submit Project\" button (bottom towards the right)\n",
    "  \n",
    "  - Otherwise, add the following files into a zip archive and submit them \n",
    "  - `helper.py`\n",
    "  - `machine_translation.ipynb`\n",
    "  - `machine_translation.html`\n",
    "    - You can export the notebook by navigating to **File -> Download as -> HTML (.html)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the html\n",
    "\n",
    "**Save your notebook before running the next cell to generate the HTML output.** Then submit your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NbConvertApp] Converting notebook machine_translation.ipynb to html',\n",
       " '[NbConvertApp] Writing 360812 bytes to machine_translation.html']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save before you run this cell!\n",
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Enhancements\n",
    "\n",
    "This project focuses on learning various network architectures for machine translation, but we don't evaluate the models according to best practices by splitting the data into separate test & training sets -- so the model accuracy is overstated. Use the [`sklearn.model_selection.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to create separate training & test datasets, then retrain each of the models using only the training set and evaluate the prediction accuracy using the hold out test set. Does the \"best\" model change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
